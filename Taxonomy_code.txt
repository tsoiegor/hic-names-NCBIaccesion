#Import libraries
from bs4 import BeautifulSoup
from selenium import webdriver
import json
import time
from urllib.request import urlopen 
import pandas as pd
from selenium.webdriver.chrome.options import Options
import numpy as np

options = webdriver.ChromeOptions()
options.add_argument("headless")

#Obtain and parse html from the main page
wd = webdriver.Chrome("C:/3D Geneomics/Task=DNAZoo JSON parsing/chromedriver-win64/chromedriver-win64/chromedriver.exe", chrome_options = options)
wd.get('https://dnazoo.s3.wasabisys.com/index.html?prefix=')
time.sleep(2)    #we need this to request elements's code from the completly loaded page. Adjust according to your internet speed
html_text = wd.execute_script("return document.body.innerHTML;")
soup = BeautifulSoup(html_text, 'lxml')
wd.close()

#Find all content with attribute <a> (contein links)
Files = soup.find_all('a')

#Create list of links to species' files. Remove other links
Links = []
for i in Files:
    i = str(i).removeprefix('<a href="')
    i = str(i).removesuffix('/</a>')
    j = -1
    while j < len(str(i)):
        if i[j] == '/':
            i = str(i).removesuffix('/')
            break
        else:
            i = str(i).removesuffix(str(i)[j])
    Links.append(i)
Links = Links[1: (-3)]
Links = np.unique(Links)

species_list_raw = open("C:/3D Geneomics/Task=NCBI accesion for species/Species_for_Egor.txt").readlines()
species_list = []
for i in species_list_raw:
    i = str(i).removesuffix("\n")
    species_list.append(i)

Output_data_table = pd.DataFrame(columns = ("Species", "SRA"))
k = 0
for Link in Links:
    try:
        dr = webdriver.Chrome("C:/3D Geneomics/Task=DNAZoo JSON parsing/chromedriver-win64/chromedriver-win64/chromedriver.exe", chrome_options=options)
        dr.get(Link)
        time.sleep(2) #we need this to request elements's code from the completly loaded page. Adjust according to your internet speed
        src = dr.execute_script("return document.body.innerHTML;")
        JSON = []   #here we will store link to json file
        soup = BeautifulSoup(src, 'lxml')
        Links_to_files = soup.find_all('a')
        for i in list(Links_to_files):
            if "README.json" in i:
                JSON.append(i)
                
        for link in JSON:
            link = str(link).removeprefix('<a href="')
            while True:
                if link[-1] == '\"':
                    link = str(link).removesuffix('\"')
                    JSON = link
                    break
                else:
                    link = str(link).removesuffix(str(link)[-1])
        print(JSON)
        dr.close()    
    
       
        for unit in species_list:
            if unit in JSON:
                print(unit)
                taxonomy = {'Aves': '174371',
                            'Reptilia': '173747',
                            'Teleostei': '161105',
                            'Pteraspidomorphi': '159750',
                            'Holostei': '161089',
                            'Dipnoi': '161039',
                            'Chondrostei': '161062',
                            'Cladistei': '1025074',
                            'Chondrichthes': '159785'}
                response = urlopen(JSON)
                data_json = json.loads(response.read())
            
                SRA_list = []
                for code in taxonomy.values():
                    if code in data_json["organism"]["taxonomy"]:
                        for i in taxonomy.keys():
                            if taxonomy[i] == code:
                                taxon = i
                        String = [unit]
                        for i in range(0, len(data_json["hicSamples"])):
                            for j in range(0, len(data_json["hicSamples"][i]["libraries"])):
                                try:
                                    Name = data_json["hicSamples"][i]["libraries"][j]["name"]
                                    dr = webdriver.Chrome("C:/3D Geneomics/Task=DNAZoo JSON parsing/chromedriver-win64/chromedriver-win64/chromedriver.exe", chrome_options = options)
                                    dr.get(f"https://www.ncbi.nlm.nih.gov/sra/?term={Name}")
                                    time.sleep(2) #we need this to request elements's code from the completly loaded page. Adjust according to your internet speed
                                    src = dr.execute_script("return document.body.innerHTML;")
                                    soup = BeautifulSoup(src, 'lxml')
                                    dr.close()
                                    SRA_raw = soup.find_all('td', {"align" : "left"})
                                    SRA_raw = str(SRA_raw[0]).removesuffix("</a></td>")
                                    SRA = SRA_raw.split(">")[-1]
                                    String.append(SRA)
                                except:
                                    Links_to_SRA = []
                                    Name = data_json["hicSamples"][i]["libraries"][j]["name"]
                                    dr = webdriver.Chrome("C:/3D Geneomics/Task=DNAZoo JSON parsing/chromedriver-win64/chromedriver-win64/chromedriver.exe", chrome_options = options)
                                    dr.get(f"https://www.ncbi.nlm.nih.gov/sra/?term={Name}")
                                    time.sleep(2) #we need this to request elements's code from the completly loaded page. Adjust according to your internet speed
                                    src = dr.execute_script("return document.body.innerHTML;")
                                    soup = BeautifulSoup(src, 'lxml')
                                    dr.close()
                                    Links_raw = soup.find_all('a', href = True)
                                    for link in Links_raw:
                                        if "sra/SR" in str(link['href']):
                                            Links_to_SRA.append("https://www.ncbi.nlm.nih.gov/" + str(link['href']))
                                    for link in Links_to_SRA:
                                        dr = webdriver.Chrome("C:/3D Geneomics/Task=DNAZoo JSON parsing/chromedriver-win64/chromedriver-win64/chromedriver.exe", chrome_options = options)
                                        dr.get(link)
                                        time.sleep(2) #we need this to request elements's code from the completly loaded page. Adjust according to your internet speed
                                        src = dr.execute_script("return document.body.innerHTML;")
                                        soup = BeautifulSoup(src, 'lxml')
                                        dr.close()
                                        SRA_raw = soup.find_all('td', {"align" : "left"})
                                        SRA_raw = str(SRA_raw[0]).removesuffix("</a></td>")
                                        SRA = SRA_raw.split(">")[-1]
                                        SRA_list.append(SRA)
                        String.append(SRA_list)
                        String.append(taxon)
                        String.append(data_json["organism"]["taxonomy"])
                        if '' in String:
                            String.remove("")
                    else:
                        continue

                            
                            
                String = pd.Series({"Species": String[0], "SRA": String[1], "Taxon": String[-2], "Taxonomy": String[-1]})
                print(String)
                Output_data_table = Output_data_table.append(String, ignore_index=True)
                del String
                del data_json
                break
                
                

    
            else:
                continue
    except KeyboardInterrupt:
        exit()
    except:
        print(f"{Link} doesn't meet requirements")

    
Output_data_table.to_csv("C:/3D Geneomics/Task=NCBI accesion for species/Data_output.csv", sep = " ")                
    
